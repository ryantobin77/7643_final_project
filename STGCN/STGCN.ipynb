{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd '/content/drive/MyDrive'\n",
        "%cd 'STGCN/'\n",
        "%cd 'STGCN-main'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtZsIn1NXpbr",
        "outputId": "2f2f702c-1f10-4fdc-ee23-b79083a680b1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive\n",
            "/content/drive/MyDrive/STGCN\n",
            "/content/drive/MyDrive/STGCN/STGCN-main\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r ./requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Ngs_4-fwyzi",
        "outputId": "4e267b71-d01c-4b6a-a7e7-eef7f057f0cf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy~=1.22.1 in /usr/local/lib/python3.10/dist-packages (from -r ./requirements.txt (line 1)) (1.22.4)\n",
            "Collecting pandas~=1.4.3\n",
            "  Downloading pandas-1.4.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit_learn~=1.1.2\n",
            "  Downloading scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy~=1.7.3\n",
            "  Downloading scipy-1.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (39.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch~=1.13.0\n",
            "  Downloading torch-1.13.1-cp310-cp310-manylinux1_x86_64.whl (887.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m887.5/887.5 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm~=4.64.0\n",
            "  Downloading tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas~=1.4.3->-r ./requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas~=1.4.3->-r ./requirements.txt (line 2)) (2022.7.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn~=1.1.2->-r ./requirements.txt (line 3)) (3.1.0)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit_learn~=1.1.2->-r ./requirements.txt (line 3)) (1.2.0)\n",
            "Collecting nvidia-cublas-cu11==11.10.3.66\n",
            "  Downloading nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.1/317.1 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.3/849.3 kB\u001b[0m \u001b[31m68.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
            "  Downloading nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.0/21.0 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch~=1.13.0->-r ./requirements.txt (line 5)) (4.5.0)\n",
            "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
            "  Downloading nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m557.1/557.1 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch~=1.13.0->-r ./requirements.txt (line 5)) (67.7.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch~=1.13.0->-r ./requirements.txt (line 5)) (0.40.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas~=1.4.3->-r ./requirements.txt (line 2)) (1.16.0)\n",
            "Installing collected packages: tqdm, scipy, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cublas-cu11, scikit_learn, pandas, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: tqdm\n",
            "    Found existing installation: tqdm 4.65.0\n",
            "    Uninstalling tqdm-4.65.0:\n",
            "      Successfully uninstalled tqdm-4.65.0\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.10.1\n",
            "    Uninstalling scipy-1.10.1:\n",
            "      Successfully uninstalled scipy-1.10.1\n",
            "  Attempting uninstall: scikit_learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.0+cu118\n",
            "    Uninstalling torch-2.0.0+cu118:\n",
            "      Successfully uninstalled torch-2.0.0+cu118\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchtext 0.15.1 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchdata 0.6.0 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 1.13.1 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas~=1.5.3, but you have pandas 1.4.4 which is incompatible.\n",
            "arviz 0.15.1 requires scipy>=1.8.0, but you have scipy 1.7.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 pandas-1.4.4 scikit_learn-1.1.3 scipy-1.7.3 torch-1.13.1 tqdm-4.64.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --epochs 100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PDrcehpyXDV2",
        "outputId": "833f0583-3169-4d34-c209-6341de64b1ff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training configs: Namespace(enable_cuda=True, seed=42, dataset='metr-la', n_his=12, n_pred=3, time_intvl=5, Kt=3, stblock_num=2, act_func='glu', Ks=3, graph_conv_type='cheb_graph_conv', gso_type='sym_norm_lap', enable_bias=True, droprate=0.5, lr=0.001, weight_decay_rate=0.0005, batch_size=32, epochs=100, opt='adam', step_size=10, gamma=0.95, patience=30)\n",
            "100% 750/750 [00:31<00:00, 23.69it/s]\n",
            "Epoch: 001 | Lr: 0.00002134373384587749 |Train loss: 0.307580 | Val loss: 0.310407 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:24<00:00, 30.22it/s]\n",
            "Epoch: 002 | Lr: 0.00000045555497448366 |Train loss: 0.250643 | Val loss: 0.301275 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:25<00:00, 29.62it/s]\n",
            "Epoch: 003 | Lr: 0.00000000972324412754 |Train loss: 0.249211 | Val loss: 0.301302 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:26<00:00, 28.22it/s]\n",
            "Epoch: 004 | Lr: 0.00000000020753033478 |Train loss: 0.249148 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:24<00:00, 30.14it/s]\n",
            "Epoch: 005 | Lr: 0.00000000000442947223 |Train loss: 0.249330 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:25<00:00, 29.08it/s]\n",
            "Epoch: 006 | Lr: 0.00000000000009454148 |Train loss: 0.249217 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:26<00:00, 28.61it/s]\n",
            "Epoch: 007 | Lr: 0.00000000000000201787 |Train loss: 0.249242 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:25<00:00, 29.73it/s]\n",
            "Epoch: 008 | Lr: 0.00000000000000004307 |Train loss: 0.249295 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:25<00:00, 29.05it/s]\n",
            "Epoch: 009 | Lr: 0.00000000000000000092 |Train loss: 0.249279 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:26<00:00, 28.78it/s]\n",
            "Epoch: 010 | Lr: 0.00000000000000000002 |Train loss: 0.249337 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:25<00:00, 29.63it/s]\n",
            "Epoch: 011 | Lr: 0.00000000000000000000 |Train loss: 0.249321 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:25<00:00, 29.73it/s]\n",
            "Epoch: 012 | Lr: 0.00000000000000000000 |Train loss: 0.249216 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:24<00:00, 30.44it/s]\n",
            "Epoch: 013 | Lr: 0.00000000000000000000 |Train loss: 0.249145 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:24<00:00, 30.49it/s]\n",
            "Epoch: 014 | Lr: 0.00000000000000000000 |Train loss: 0.249364 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:25<00:00, 29.09it/s]\n",
            "Epoch: 015 | Lr: 0.00000000000000000000 |Train loss: 0.249253 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:24<00:00, 30.55it/s]\n",
            "Epoch: 016 | Lr: 0.00000000000000000000 |Train loss: 0.249249 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:24<00:00, 30.12it/s]\n",
            "Epoch: 017 | Lr: 0.00000000000000000000 |Train loss: 0.249337 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:24<00:00, 30.80it/s]\n",
            "Epoch: 018 | Lr: 0.00000000000000000000 |Train loss: 0.249223 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:23<00:00, 31.40it/s]\n",
            "Epoch: 019 | Lr: 0.00000000000000000000 |Train loss: 0.249283 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:25<00:00, 29.70it/s]\n",
            "Epoch: 020 | Lr: 0.00000000000000000000 |Train loss: 0.249393 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:24<00:00, 31.23it/s]\n",
            "Epoch: 021 | Lr: 0.00000000000000000000 |Train loss: 0.249371 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:24<00:00, 30.91it/s]\n",
            "Epoch: 022 | Lr: 0.00000000000000000000 |Train loss: 0.249291 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:25<00:00, 29.95it/s]\n",
            "Epoch: 023 | Lr: 0.00000000000000000000 |Train loss: 0.249122 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:24<00:00, 30.36it/s]\n",
            "Epoch: 024 | Lr: 0.00000000000000000000 |Train loss: 0.249334 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:25<00:00, 28.93it/s]\n",
            "Epoch: 025 | Lr: 0.00000000000000000000 |Train loss: 0.249346 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:25<00:00, 29.47it/s]\n",
            "Epoch: 026 | Lr: 0.00000000000000000000 |Train loss: 0.249317 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:24<00:00, 30.90it/s]\n",
            "Epoch: 027 | Lr: 0.00000000000000000000 |Train loss: 0.249371 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:24<00:00, 30.38it/s]\n",
            "Epoch: 028 | Lr: 0.00000000000000000000 |Train loss: 0.249292 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:24<00:00, 31.08it/s]\n",
            "Epoch: 029 | Lr: 0.00000000000000000000 |Train loss: 0.249199 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:25<00:00, 30.00it/s]\n",
            "Epoch: 030 | Lr: 0.00000000000000000000 |Train loss: 0.249426 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:24<00:00, 30.82it/s]\n",
            "Epoch: 031 | Lr: 0.00000000000000000000 |Train loss: 0.249164 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "100% 750/750 [00:24<00:00, 30.39it/s]\n",
            "Epoch: 032 | Lr: 0.00000000000000000000 |Train loss: 0.249210 | Val loss: 0.301305 | GPU occupy: 678.569984 MiB\n",
            "Early stopping.\n",
            "Dataset metr-la | Test loss 0.269065 | MAE 4.474542 | RMSE 9.573143 | WMAPE 0.08808351\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main.py --epochs 100 --dataset 'pemsd7-m'"
      ],
      "metadata": {
        "id": "Qt3JkgPfw1VA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c816a4e-d275-4914-e4bd-e12a01704337"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training configs: Namespace(enable_cuda=True, seed=42, dataset='pemsd7-m', n_his=12, n_pred=3, time_intvl=5, Kt=3, stblock_num=2, act_func='glu', Ks=3, graph_conv_type='cheb_graph_conv', gso_type='sym_norm_lap', enable_bias=True, droprate=0.5, lr=0.001, weight_decay_rate=0.0005, batch_size=32, epochs=100, opt='adam', step_size=10, gamma=0.95, patience=30)\n",
            "100% 277/277 [00:09<00:00, 27.93it/s]\n",
            "Epoch: 001 | Lr: 0.00025034408974245479 |Train loss: 0.311898 | Val loss: 0.203802 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:07<00:00, 34.95it/s]\n",
            "Epoch: 002 | Lr: 0.00005953855510552937 |Train loss: 0.191793 | Val loss: 0.193840 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 33.31it/s]\n",
            "Epoch: 003 | Lr: 0.00001415986911335100 |Train loss: 0.182467 | Val loss: 0.203354 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:07<00:00, 34.73it/s]\n",
            "Epoch: 004 | Lr: 0.00000354483954405416 |Train loss: 0.180244 | Val loss: 0.207837 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 33.92it/s]\n",
            "Epoch: 005 | Lr: 0.00000084305814749233 |Train loss: 0.179420 | Val loss: 0.210346 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 34.12it/s]\n",
            "Epoch: 006 | Lr: 0.00000020050189330723 |Train loss: 0.179483 | Val loss: 0.210749 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 34.40it/s]\n",
            "Epoch: 007 | Lr: 0.00000005019446397164 |Train loss: 0.178942 | Val loss: 0.210850 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 34.29it/s]\n",
            "Epoch: 008 | Lr: 0.00000001193759302344 |Train loss: 0.179188 | Val loss: 0.210878 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 34.30it/s]\n",
            "Epoch: 009 | Lr: 0.00000000283908056621 |Train loss: 0.179450 | Val loss: 0.210885 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 34.17it/s]\n",
            "Epoch: 010 | Lr: 0.00000000067520968805 |Train loss: 0.179355 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 34.33it/s]\n",
            "Epoch: 011 | Lr: 0.00000000016903475474 |Train loss: 0.179452 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 34.40it/s]\n",
            "Epoch: 012 | Lr: 0.00000000004020100922 |Train loss: 0.179427 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 34.06it/s]\n",
            "Epoch: 013 | Lr: 0.00000000000956088081 |Train loss: 0.179145 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 34.23it/s]\n",
            "Epoch: 014 | Lr: 0.00000000000239351000 |Train loss: 0.179507 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 33.83it/s]\n",
            "Epoch: 015 | Lr: 0.00000000000056924103 |Train loss: 0.179199 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:07<00:00, 34.65it/s]\n",
            "Epoch: 016 | Lr: 0.00000000000013538082 |Train loss: 0.179236 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 34.30it/s]\n",
            "Epoch: 017 | Lr: 0.00000000000003389179 |Train loss: 0.179067 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 33.49it/s]\n",
            "Epoch: 018 | Lr: 0.00000000000000806038 |Train loss: 0.179149 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 34.61it/s]\n",
            "Epoch: 019 | Lr: 0.00000000000000191697 |Train loss: 0.179140 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 34.00it/s]\n",
            "Epoch: 020 | Lr: 0.00000000000000045591 |Train loss: 0.179158 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 33.94it/s]\n",
            "Epoch: 021 | Lr: 0.00000000000000011413 |Train loss: 0.179238 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:07<00:00, 34.72it/s]\n",
            "Epoch: 022 | Lr: 0.00000000000000002714 |Train loss: 0.179578 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 34.13it/s]\n",
            "Epoch: 023 | Lr: 0.00000000000000000646 |Train loss: 0.179241 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 34.40it/s]\n",
            "Epoch: 024 | Lr: 0.00000000000000000162 |Train loss: 0.179394 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 34.33it/s]\n",
            "Epoch: 025 | Lr: 0.00000000000000000038 |Train loss: 0.179443 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 33.86it/s]\n",
            "Epoch: 026 | Lr: 0.00000000000000000009 |Train loss: 0.179457 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 34.05it/s]\n",
            "Epoch: 027 | Lr: 0.00000000000000000002 |Train loss: 0.179164 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 33.96it/s]\n",
            "Epoch: 028 | Lr: 0.00000000000000000001 |Train loss: 0.179393 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 34.45it/s]\n",
            "Epoch: 029 | Lr: 0.00000000000000000000 |Train loss: 0.179430 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 34.03it/s]\n",
            "Epoch: 030 | Lr: 0.00000000000000000000 |Train loss: 0.179209 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:08<00:00, 34.17it/s]\n",
            "Epoch: 031 | Lr: 0.00000000000000000000 |Train loss: 0.179538 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "100% 277/277 [00:07<00:00, 34.75it/s]\n",
            "Epoch: 032 | Lr: 0.00000000000000000000 |Train loss: 0.179150 | Val loss: 0.210886 | GPU occupy: 413.910016 MiB\n",
            "Early stopping.\n",
            "Dataset pemsd7-m | Test loss 0.209191 | MAE 3.148914 | RMSE 5.168435 | WMAPE 0.05422127\n"
          ]
        }
      ]
    }
  ]
}